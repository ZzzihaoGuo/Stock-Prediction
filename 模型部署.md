## 1、Deepseek 相关模型及其性能
deepseek-r1
|模型大小|模型特点|
|---|---|
|1.5B|处理基础nlp任务（聊天机器人）|
|7bB|中等复杂度的字眼语言任务|
|8B-14B|平衡性能与成本的主力模型|
|14B-70B|处理复杂语义理解和生成任务|
|671bB|面向科研机构和大规模云服务的高复杂度任务|
## 2、Deepseek 部署的硬件要求
内存和显存相加需要满足xxx，部署大模型不完全要求显卡，只要内存和显存大小满足要求即可。只有内存的话推理速度会有一定限制，但是如果使用规模不大的情况下完全可以满足要求。
### （1）不满血硬件要求
1. 小模型部署
1.5B-7B的模型最低cpu可以部署，无需gpu
	**1.5B模型**：CPU最低4核，内存8GB+，硬盘icon3GB+存储空间。若GPU加速可选4GB+显存
	**7B模型**：CPU 8核以上，内存16GB+，硬盘8GB+，显卡推荐8GB+显存
14B-70B模型需要GPU部署（高端苹果的cpu也可以部署）
	**14B模型**：CPU 12核以上，内存32GB+，硬盘15GB+，显卡16GB+显存
	**32B模型**：CPU 16核以上，内存64GB+，硬盘30GB+，显卡24GB+显存
	**70B模型**：CPU 32核以上，内存128GB+，硬盘70GB+，显卡需多卡并行（A100）
	70B模型通常需要4×A100 80G，但是但是经过4bit或者动态bit量化，最低可在单张A100 40G运行
2. 不满血671B大模型部署经过4bit或者动态bit量化，最低需要内存＋显存＞200G，单台高端苹果电脑或4×4090可以部署。

### （2）满血硬件要求
1. 8bit量化全量参数671b模型可以做到满血，至少需要8×A800 80G 服务器 
## 3、Deepseek 部署技术方案
### （1）模型部署技术方案
本方案采用ollama开源框架部署：
1. 1.5-8B模型，在硬件资源足够的场景下，推荐采用fp16或8bit量化，获得全量效果。
1. 8B以上模型模型资源足够条件才同1.5-8B模型，但是考虑到可能资源受限，可以采用4bit~1.58bit量化。
**具体方案流如下：**
1. 下载开源的deepseek-r1模型文件
1. 安装ollama
1. 创建Modelfile文件，用于指导ollama建立模型
1. 创建ollama模型
1. 运行模型
1. 安装设计web界面
### （2）模型＋RAG部署技术方案
1. 安装MaxKB
1. 在MaxKB中配置deepseek模型
1. 加入所需文档，使用向量模型创建知识库
1. 测试应用是否正确
### （3）模型＋RAG＋联网技术方案

